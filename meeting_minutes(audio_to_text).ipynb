{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+QIzn0lIQB2d3kuDM79RW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soroushav/llm_basics/blob/main/meeting_minutes(audio_to_text).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_z3n_pvJpbvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cGrEMz-jWpx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive, userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA = \"meta-llama/Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "E2a9YPTQkxqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "audio_file = '/content/drive/MyDrive/llm_basics/denver_extract.mp3'"
      ],
      "metadata": {
        "id": "OHaVr39Rkz7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "tRbhKzExl43J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "6ac01dysnIwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = open(audio_file, \"rb\")"
      ],
      "metadata": {
        "id": "pMiMOskJnX6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MODEL = \"gpt-4o-mini-transcribe\"\n",
        "transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "s-Jggph_ndQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(transcription))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tgTavFkioCJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "You produce minutes of meetings from transcripts, with summary, key discussion points,\n",
        "takeaways and action items with owners, in markdown format without code blocks.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = f\"\"\"\n",
        "Below is an extract transcript of a Denver council meeting.\n",
        "Please write minutes in markdown without code blocks, including:\n",
        "- a summary with attendees, location and date\n",
        "- discussion points\n",
        "- takeaways\n",
        "- action items with owners\n",
        "\n",
        "Transcription:\n",
        "{transcription}\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "nrlvrySSoWii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "IXSQjkJloin4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache_dir= \"/content/drive/MyDrive/llm_basics/hf_cache\""
      ],
      "metadata": {
        "id": "YCsUkFNiGTE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA, cache_dir=cache_dir)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, cache_dir=cache_dir, quantization_config=quant_config, device_map=\"auto\")\n",
        "model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n"
      ],
      "metadata": {
        "id": "0tqsruZQoqrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "import threading"
      ],
      "metadata": {
        "id": "RCiHXpe-5HMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_stream_optimized(user_input):\n",
        "  global tokenizer, model, messages\n",
        "  print(tokenizer)\n",
        "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  streamer = TextIteratorStreamer(tokenizer,\n",
        "                                  skip_prompt=True,\n",
        "                                  decode_kwargs={\"skip_special_tokens\": True})\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,\n",
        "      kwargs={\n",
        "          \"inputs\": inputs,\n",
        "          \"max_new_tokens\": 2000,\n",
        "          \"streamer\": streamer\n",
        "          }\n",
        "  )\n",
        "  thread.start()\n",
        "  response = \"\"\n",
        "  for chunk in streamer:\n",
        "    chunk.replace(\"<|eot_id|>\", \" \")\n",
        "    response += chunk\n",
        "    yield response\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "-SGUgqoYqXQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "YPy0Cr9S6vj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = messages[:2]"
      ],
      "metadata": {
        "id": "rK0WVUIq_JEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chat with AI (Streaming Enabled)\")\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        user_input = gr.Textbox(label=\"Your message\", placeholder=\"Type something...\")\n",
        "        output_box = gr.Markdown(label=\"AI Response\", min_height=50)\n",
        "        send_button = gr.Button(\"Send\")\n",
        "\n",
        "    send_button.click(fn=generate_stream_optimized, inputs=user_input, outputs=output_box)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "7UrABWU56xuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAZCe1ic8V1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXlK4ybj97N7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}